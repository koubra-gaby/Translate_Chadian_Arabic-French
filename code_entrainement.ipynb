{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad286c81",
   "metadata": {},
   "source": [
    "# Les importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd937727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install evaluate\n",
    "%pip install sacrebleu rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les importations\n",
    "from transformers import get_linear_schedule_with_warmup, AutoTokenizer,TrainerCallback,AutoModelForCausalLM, DataCollatorForSeq2Seq,AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,EarlyStoppingCallback\n",
    "from datasets import load_dataset,Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab73c2b9",
   "metadata": {},
   "source": [
    "### Chargement du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1795c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notre fichier d'arabe\n",
    "#with open(\"/kaggle/input/arb-fr-2/arabe_tchad3.txt\", \"r\") as file:\n",
    "with open(\"for_dataset_arb.txt\", \"r\") as file:\n",
    "    # On lit le contenu du fichier\n",
    "    data_arb = file.readlines()\n",
    "# notre fichier de fran√ßais\n",
    "#with open(\"/kaggle/input/arb-fr-2/V_francais_3.txt\", \"r\") as file:\n",
    "with open(\"for_dataset_fr.txt\", \"r\") as file:\n",
    "    # On lit le contenu du fichier\n",
    "    data_fr = file.readlines()\n",
    "\n",
    "# Supprimons les lignes vides\n",
    "data_fr = [line for line in data_fr if line.strip()]\n",
    "data_arb = [line for line in data_arb if line.strip()]\n",
    "print(\"fr_lengh :\", len(data_fr))\n",
    "print(\"arb_lengh :\", len(data_arb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499d9c1",
   "metadata": {},
   "source": [
    "### Division en train, test, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e79ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du dictionnaire\n",
    "dataset_dict = {\"translation\": [{\"arb\": a.strip(), \"fr\": f.strip()} for a, f in zip(data_arb, data_fr)]}\n",
    "\n",
    "# Cr√©ation du dataset\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "# On va diviser notre dataset en deux parties\n",
    " #train_dataset =  DatasetDict({\"train\":test['train']})\n",
    "\n",
    "train_dataset = dataset.train_test_split(test_size=0.1)\n",
    "# On entregistre notre dataset\n",
    "#save_path = \"dataset_arabe_fr\"\n",
    "#train_dataset.save_to_disk(save_path)\n",
    "# On va charger notre dataset\n",
    "#train_dataset = DatasetDict.load_from_disk(save_path)\n",
    "#train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60237f46",
   "metadata": {},
   "source": [
    "# L'entrainement pour la traduction de l'arabe vers le fran√ßais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7edf310",
   "metadata": {},
   "source": [
    "### La definition des langue cible et d'entr√©e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ea8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_langage(examples) :\n",
    "    # On va extraire le langage arabe et le langage fran√ßais\n",
    "    inputs = [arb['arb'].strip() for arb in examples[\"translation\"]] \n",
    "    target = [fr['fr'].strip()  for fr in examples[\"translation\"]]\n",
    "    return {'inputs' : inputs, 'target' : target}\n",
    "\n",
    "# On va appliquer la fonction sur notre dataset\n",
    "train_dataset = train_dataset.map(extract_langage, batched=True, remove_columns=[\"translation\"])\n",
    "print(train_dataset[\"train\"][\"target\"][0])\n",
    "print(train_dataset['train'][\"inputs\"][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44291b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09243527",
   "metadata": {},
   "source": [
    "### Evaluation du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les m√©triques\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "#accuracy_metric = evaluate.load(\"accuracy\")  # Pour √©valuer la pr√©cision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d93d7",
   "metadata": {},
   "source": [
    "### La definition des metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0322bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]  # ROUGE/METEOR attend des listes imbriqu√©es\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # D√©codage des pr√©dictions et labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage du texte\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # üèÜ Calcul des m√©triques\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    meteor_result = meteor_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # accuracy_result = accuracy_metric.compute(predictions=decoded_preds, references=decoded_labels) # Removed accuracy metric\n",
    "\n",
    "    # üìä R√©cup√©ration des scores\n",
    "    result = {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"rouge\": rouge_result[\"rougeL\"],  # Utilisation du score ROUGE-L\n",
    "        \"meteor\": meteor_result[\"meteor\"],\n",
    "        # \"accuracy\": accuracy_result[\"accuracy\"]  # Pr√©cision des pr√©dictions # Removed accuracy metric\n",
    "    }\n",
    "\n",
    "    # Calcul de la longueur moyenne des pr√©dictions\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # Arrondir les scores\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814940bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8562a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "early =  EarlyStoppingCallback(early_stopping_patience = 5)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()  # Lib√©rer la m√©moire GPU inutilis√©e\n",
    "\n",
    "\n",
    "class AMPCallback(TrainerCallback):\n",
    "    def __init__(self):  #torch.amp.GradScaler('cuda', args...)\n",
    "        super().__init__()\n",
    "        self.scaler = GradScaler('cuda')  # Initialise le scaler pour AMP\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        control.should_log = True  # Forcer l'affichage des logs\n",
    "\n",
    "    def on_backward_end(self, args, state, control, **kwargs):\n",
    "        with autocast():  # Active AMP pour les calculs\n",
    "            self.scaler.scale(state.loss).backward()\n",
    "            self.scaler.step(state.optimizer)\n",
    "            self.scaler.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, token = os.environ.get('TOKEN'))\n",
    "\n",
    "# Appliquons la tokenisation sur notre dataset\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "# Dans votre code pour Arabe Tchadien (Latin) -> Fran√ßais\n",
    "SOURCE_LANG_CODE_NLLB = \"arb_Latn\" \n",
    "TARGET_LANG_CODE_NLLB = \"fra_Latn\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.src_lang = SOURCE_LANG_CODE_NLLB\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"inputs\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Nettoyer les cibles : enlever None ou cha√Ænes vides\n",
    "    clean_targets = []\n",
    "    for t in examples[\"target\"]:\n",
    "        if isinstance(t, str) and t.strip():\n",
    "            clean_targets.append(t)\n",
    "        else:\n",
    "            clean_targets.append(\" \")\n",
    "\n",
    "    tokenizer.tgt_lang = TARGET_LANG_CODE_NLLB\n",
    "    labels = tokenizer(\n",
    "        clean_targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        text_target=clean_targets  # Pour compatibilit√© future\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "# On va appliquer la fonction sur notre dataset\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"inputs\", \"target\"])\n",
    "\n",
    "# On va maintenant definir notre collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63991c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/my_train_model_facebook_nllb_200\",\n",
    "    #run_name=\"nllb_experiment_v1\",   Nom de run distinct\n",
    "    report_to=\"none\",\n",
    "    hub_model_id= os.environ.get('HUB_ID'),  # ‚úÖ Ajoute ton Repo ID\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    #learning_rate=1e-5,\n",
    "    gradient_checkpointing=True,  # ‚úÖ Active la r√©duction m√©moire\n",
    "    #use_cache = False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=50,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True #for XPU\n",
    "    push_to_hub=True,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"steps\",\n",
    "    #save_steps = 500,\n",
    "    #resume_from_checkpoint=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"class CheckpointCleanupCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        clean_old_checkpoints(args.output_dir, keep=10)  # ‚úÖ Supprime les anciens checkpoints \"\"\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset[\"train\"],\n",
    "    eval_dataset=train_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early, AMPCallback(),MemoryCleanupCallback() ], #CheckpointCleanupCallback()\n",
    ")\n",
    "torch.cuda.empty_cache()  # Lib√©rer la m√©moire GPU inutilis√©e\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf690b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va maintenant evaluer notre mod√®le\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0dbab",
   "metadata": {},
   "source": [
    "### Telechargement de mon modele du hub et enfin l'evaluer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4dbcd",
   "metadata": {},
   "source": [
    "### Testons notre modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcba916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fait un test en traduisant une phrase\n",
    "text_to_translate = \"ju sawani.\"  # A Remplacer par la phrase que vous voulez traduire\n",
    "\n",
    "# --- 1. D√©finir les codes de langue NLLB utilis√©s lors de l'entra√Ænement ---\n",
    "# Pour l'arabe tchadien en script latin, votre tokenizer par d√©faut utilisait \"eng_Latn\" comme src_lang.\n",
    "SOURCE_LANG_CODE_NLLB = \"arb_Latn\" # C'est le code proxy que votre mod√®le a appris √† associer √† l'arabe tchadien (latin)\n",
    "TARGET_LANG_CODE_NLLB = \"fra_Latn\" # Le code NLLB pour le fran√ßais\n",
    "\n",
    "# --- 2. Votre phrase √† traduire ---\n",
    "#text_to_translate = \"Inti maalak?\"  # Phrase en arabe tchadien (latin)\n",
    "\n",
    "# --- 3. Charger le tokenizer et le mod√®le ---\n",
    "model_name = os.environ.get('HUB_ID')  # Votre mod√®le affin√©\n",
    "\n",
    "# Assurez-vous que c'est bien la version \"fast\" qui est charg√©e si c'est ce qui est par d√©faut pour votre environnement\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# --- 4. Configurer le tokenizer pour la langue source ---\n",
    "# C'est crucial : indique au tokenizer d'ajouter le bon token de langue source.\n",
    "tokenizer.src_lang = SOURCE_LANG_CODE_NLLB\n",
    "\n",
    "# --- 5. Tokeniser l'entr√©e ---\n",
    "# Le tokenizer ajoutera automatiquement le token __eng_Latn__ au d√©but de la s√©quence.\n",
    "inputs = tokenizer(text_to_translate, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pour la v√©rification (optionnel) :\n",
    "print(\"Tokens d'entr√©e (IDs) :\", inputs[\"input_ids\"][0].tolist())\n",
    "print(\"Tokens d'entr√©e d√©cod√©s :\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "\n",
    "# --- 6. G√©n√©rer la traduction ---\n",
    "# Correction ici pour obtenir l'ID du token de langue cible\n",
    "# On pr√©fixe le code de langue NLLB avec \"__\" et on le suffixe avec \"__\"\n",
    "# pour qu'il corresponde √† la forme des tokens de langue NLLB (par exemple, \"__fra_Latn__\").\n",
    "target_lang_token_id = tokenizer.convert_tokens_to_ids(f\"__{TARGET_LANG_CODE_NLLB}__\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=target_lang_token_id, # Utilisation de l'ID corrig√©\n",
    "    max_new_tokens=128 # Limitez la longueur de la traduction si n√©cessaire\n",
    ")\n",
    "\n",
    "# --- 7. D√©coder la traduction ---\n",
    "translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nLa traduction est :\", translation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3451519a",
   "metadata": {},
   "source": [
    "# La traduction du Fran√ßais vers l'arabe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ecc3b",
   "metadata": {},
   "source": [
    "### Chargement de notre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88da821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_langage(examples) :\n",
    "    # On va extraire le langage arabe et le langage fran√ßais\n",
    "    inputs = [fr['fr'] for fr in examples[\"translation\"]]\n",
    "    target = [arb['arb'] for arb in examples[\"translation\"]]\n",
    "    return {'inputs' : inputs, 'target' : target}\n",
    "\n",
    "# On va appliquer la fonction sur notre dataset\n",
    "train_dataset = train_dataset.map(extract_langage, batched=True, remove_columns=[\"translation\"])\n",
    "print(train_dataset[\"train\"][\"target\"][0])\n",
    "print(train_dataset['train'][\"inputs\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54033049",
   "metadata": {},
   "source": [
    "### Pretraitement de notre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60eab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa171d",
   "metadata": {},
   "source": [
    "### Evaluation du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les m√©triques\n",
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "#accuracy_metric = evaluate.load(\"accuracy\")  # Pour √©valuer la pr√©cision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31751726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]  # ROUGE/METEOR attend des listes imbriqu√©es\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # D√©codage des pr√©dictions et labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Nettoyage du texte\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    # üèÜ Calcul des m√©triques\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    meteor_result = meteor_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    # accuracy_result = accuracy_metric.compute(predictions=decoded_preds, references=decoded_labels) # Removed accuracy metric\n",
    "\n",
    "    # üìä R√©cup√©ration des scores\n",
    "    result = {\n",
    "        \"bleu\": bleu_result[\"score\"],\n",
    "        \"rouge\": rouge_result[\"rougeL\"],  # Utilisation du score ROUGE-L\n",
    "        \"meteor\": meteor_result[\"meteor\"],\n",
    "        # \"accuracy\": accuracy_result[\"accuracy\"]  # Pr√©cision des pr√©dictions # Removed accuracy metric\n",
    "    }\n",
    "\n",
    "    # Calcul de la longueur moyenne des pr√©dictions\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    # Arrondir les scores\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early =  EarlyStoppingCallback(early_stopping_patience = 5)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "class MemoryCleanupCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()  # Lib√©rer la m√©moire GPU inutilis√©e\n",
    "\n",
    "\n",
    "class AMPCallback(TrainerCallback):\n",
    "    def __init__(self):  #torch.amp.GradScaler('cuda', args...)\n",
    "        super().__init__()\n",
    "        self.scaler = GradScaler('cuda')  # Initialise le scaler pour AMP\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        control.should_log = True  # Forcer l'affichage des logs\n",
    "\n",
    "    def on_backward_end(self, args, state, control, **kwargs):\n",
    "        with autocast():  # Active AMP pour les calculs\n",
    "            self.scaler.scale(state.loss).backward()\n",
    "            self.scaler.step(state.optimizer)\n",
    "            self.scaler.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bd1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, token = os.environ.get('TOKEN'))\n",
    "\n",
    "# Appliquons la tokenisation sur notre dataset\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "\n",
    "# Dans votre code pour Arabe Tchadien (Latin) -> Fran√ßais\n",
    "SOURCE_LANG_CODE_NLLB = \"fra_Latn\"\n",
    "TARGET_LANG_CODE_NLLB = \"arb_Latn\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    tokenizer.src_lang = SOURCE_LANG_CODE_NLLB\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"inputs\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Nettoyer les cibles : enlever None ou cha√Ænes vides\n",
    "    clean_targets = []\n",
    "    for t in examples[\"target\"]:\n",
    "        if isinstance(t, str) and t.strip():\n",
    "            clean_targets.append(t)\n",
    "        else:\n",
    "            clean_targets.append(\" \")\n",
    "\n",
    "    tokenizer.tgt_lang = TARGET_LANG_CODE_NLLB\n",
    "    labels = tokenizer(\n",
    "        clean_targets,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        text_target=clean_targets  # Pour compatibilit√© future\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "# On va appliquer la fonction sur notre dataset\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"inputs\", \"target\"])\n",
    "\n",
    "# On va maintenant definir notre collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a882f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/my_train_model_facebook_nllb_200\",\n",
    "    #run_name=\"nllb_experiment_v1\",   Nom de run distinct\n",
    "    report_to=\"none\",\n",
    "    hub_model_id= os.environ.get('HUB_ID_2'),  # ‚úÖ Ajoute ton Repo ID\n",
    "    eval_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    #learning_rate=1e-5,\n",
    "    gradient_checkpointing=True,  # ‚úÖ Active la r√©duction m√©moire\n",
    "    #use_cache = False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=50,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, #change to bf16=True #for XPU\n",
    "    push_to_hub=True,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    save_strategy=\"steps\",\n",
    "    #save_steps = 500,\n",
    "    #resume_from_checkpoint=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"class CheckpointCleanupCallback(TrainerCallback):\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        clean_old_checkpoints(args.output_dir, keep=10)  # ‚úÖ Supprime les anciens checkpoints \"\"\"\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset[\"train\"],\n",
    "    eval_dataset=train_dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early, AMPCallback(),MemoryCleanupCallback() ], #CheckpointCleanupCallback()\n",
    ")\n",
    "torch.cuda.empty_cache()  # Lib√©rer la m√©moire GPU inutilis√©e\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73019bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9503117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va maintenant evaluer notre mod√®le\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0987fba",
   "metadata": {},
   "source": [
    "### Telechargement de mon modele du hub et enfin l'evaluer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf85d93",
   "metadata": {},
   "source": [
    "### Testons notre modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed30153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fait un test en traduisant une phrase\n",
    "text_to_translate = \"Il m'√©tait arriv√© de m'√©garer \"  # A Remplacer par la phrase que vous voulez traduire\n",
    "# on fait un test en traduisant une phrase\n",
    "\n",
    "# --- 1. D√©finir les codes de langue NLLB utilis√©s lors de l'entra√Ænement ---\n",
    "# Pour l'arabe tchadien en script latin, votre tokenizer par d√©faut utilisait \"eng_Latn\" comme src_lang.\n",
    "SOURCE_LANG_CODE_NLLB = \"fra_Latn\" # C'est le code proxy que votre mod√®le a appris √† associer √† l'arabe tchadien (latin)\n",
    "TARGET_LANG_CODE_NLLB = \"arb_Latn\"# Le code NLLB pour le fran√ßais\n",
    "\n",
    "# --- 2. Votre phrase √† traduire ---\n",
    "#text_to_translate = \"Inti maalak?\"  # Phrase en arabe tchadien (latin)\n",
    "\n",
    "# --- 3. Charger le tokenizer et le mod√®le ---\n",
    "model_name = os.environ.get(\"HUB_ID_2\")  # Remplacez par le nom de votre mod√®le \n",
    "\n",
    "# Assurez-vous que c'est bien la version \"fast\" qui est charg√©e si c'est ce qui est par d√©faut pour votre environnement\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# --- 4. Configurer le tokenizer pour la langue source ---\n",
    "# C'est crucial : indique au tokenizer d'ajouter le bon token de langue source.\n",
    "tokenizer.src_lang = SOURCE_LANG_CODE_NLLB\n",
    "\n",
    "# --- 5. Tokeniser l'entr√©e ---\n",
    "# Le tokenizer ajoutera automatiquement le token __eng_Latn__ au d√©but de la s√©quence.\n",
    "inputs = tokenizer(text_to_translate, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pour la v√©rification (optionnel) :\n",
    "print(\"Tokens d'entr√©e (IDs) :\", inputs[\"input_ids\"][0].tolist())\n",
    "print(\"Tokens d'entr√©e d√©cod√©s :\", tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]))\n",
    "\n",
    "# --- 6. G√©n√©rer la traduction ---\n",
    "# Correction ici pour obtenir l'ID du token de langue cible\n",
    "# On pr√©fixe le code de langue NLLB avec \"__\" et on le suffixe avec \"__\"\n",
    "# pour qu'il corresponde √† la forme des tokens de langue NLLB (par exemple, \"__fra_Latn__\").\n",
    "target_lang_token_id = tokenizer.convert_tokens_to_ids(f\"__{TARGET_LANG_CODE_NLLB}__\")\n",
    "#print(target_lang_token_id)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    forced_bos_token_id=target_lang_token_id, # Utilisation de l'ID corrig√©\n",
    "    max_new_tokens=256 # Limitez la longueur de la traduction si n√©cessaire\n",
    ")\n",
    "\n",
    "# --- 7. D√©coder la traduction ---\n",
    "translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nLa traduction est :\", translation[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".envIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
